pytorch==2.1.0

# Transformers and ML Core
transformers>=4.40.0            # AutoModelForCausalLM, AutoTokenizer, TrainingArguments
datasets>=2.15.0                # load_dataset
accelerate>=0.25.0              # Distributed training and model loading
huggingface_hub>=0.20.0         # Model hub upload/download operations

# Fine-tuning Specialized Libraries
peft>=0.10.0                    # Parameter-Efficient Fine-Tuning
trl>=0.8.0                      # Transformers Reinforcement Learning

# Experiment Tracking
wandb>=0.16.0                   # Weights & Biases for experiment monitoring

# Performance Optimization - A100/H100 GPU optimization
flash-attn>=2.3.0               # Flash Attention for memory efficiency
